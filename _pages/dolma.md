---
permalink: /dolma/
# redirect_to:
---

Paper coming soon!

<br>*Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research*
<br><sub>Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, **Ian Magnusson**, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Evan Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo</sub>
<br><sub> In preparation  // 
[[paper](https://arxiv.org/abs/2402.00159)] [[data](https://huggingface.co/datasets/allenai/dolma#:~:text=Dolma%20is%20a%20dataset%20of,as%20a%20medium%20risk%20artifact.)] [[code](https://github.com/allenai/dolma)] [[blog](https://blog.allenai.org/dolma-3-trillion-tokens-open-llm-corpus-9a0ff4b8da64)] [[press](https://techcrunch.com/2023/08/18/ai2-drops-biggest-open-dataset-yet-for-training-language-models/)]</sub>