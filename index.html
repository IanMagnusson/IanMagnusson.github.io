---
# You don't need to edit this file, it's empty on purpose.
# Edit theme's home layout instead if you wanna make some changes
# See: https://jekyllrb.com/docs/themes/#overriding-theme-defaults
layout: single
author_profile: true
--- 
Hi, I'm Ian! Do AI research at <a href="https://cs.washington.edu">University of Washington</a> and the <a href="https://allenai.org">Allen Institute for AI</a> with <a href="https://nasmith.github.io">Noah Smith</a> and <a href="https://koh.pw">Pang Wei Koh</a>. 


I'm interested in the science of language modeling, especially advancing evaluation to better understand scaling behavior and robustness across textual domains.

<br><br>

Previously, I was a <a href="https://allenai.org/predoctoral-young-investigators">PYI</a> at <a href="https://allenai.org">AI2</a>. I got my MS in computer science from <a href="https://www.khoury.northeastern.edu/">Northeastern University</a>, and interned at <a href="https://aws.amazon.com/about-aws/">AWS</a> AI Labs and <a href="https://www.sift.net">SIFT</a>. I also hold a BA in cultural anthropology from <a href="https://www.bard.edu">Bard College</a>.

<h2>Selected Publications</h2>

<br><em>Paloma: A Benchmark for Evaluating Language Model Fit</em>
<br><sub><strong>Ian Magnusson</strong>, Akshita Bhagia, Valentin Hofmann, Luca Soldaini, Ananya Harsh Jha, Oyvind Tafjord, Dustin Schwenk, Evan Pete Walsh, Yanai Elazar, Kyle Lo, Dirk Groeneveld, Iz Beltagy,
Hannaneh Hajishirzi, Noah A. Smith, Kyle Richardson, Jesse Dodge</sub>
<br><sub> arxiv  // <a href="https://arxiv.org/abs/2312.10523">[paper]</a> <a href="https://huggingface.co/datasets/allenai/paloma">[data]</a> <a href="https://github.com/allenai/ai2-olmo-eval/blob/main/paloma/README.md">[code]</a> <a href="https://huggingface.co/collections/allenai/paloma-6580bf704daa78a2f2668838">[models]</a></sub>

<br><br>

<br><em>OLMo: Accelerating the Science of Language Models</em>
<br><sub>Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, <strong>Ian Magnusson</strong>, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, et al.</sub>
<br><sub> ACL 2024 // <a href="https://arxiv.org/abs/2402.00838">[paper]</a> <a href="https://huggingface.co/allenai/OLMo-7B">[model]</a> <a href="https://github.com/allenai/olmo">[code]</a> <a href="https://blog.allenai.org/olmo-open-language-model-87ccfc95f580">[blog]</a> <a href="https://www.axios.com/2024/02/01/allen-institute-for-ai-fully-open-source-large-language-model-olmo-7b">[press]</a></sub>
<!-- <br><sub><a href="https://x.com/aclmeeting/status/1823664612677705762">[Best Theme Paper Award]</a></sub> -->
<br>
<sub>
  <a href="https://x.com/aclmeeting/status/1823664612677705762">
    <img src="https://img.shields.io/badge/Best%20Theme%20Paper%20Award-%F0%9F%8F%86-ADD8E6" alt="Best Theme Paper Award">
  </a>
</sub>

<br><br>

<br><em>Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research</em>
<br><sub>Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, <strong>Ian Magnusson</strong>, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, et al.</sub>
<br><sub> ACL 2024  // <a href="https://arxiv.org/abs/2402.00159">[paper]</a> <a href="https://huggingface.co/datasets/allenai/dolma#:~:text=Dolma%20is%20a%20dataset%20of,as%20a%20medium%20risk%20artifact.">[data]</a> <a href="https://github.com/allenai/dolma">[code]</a> <a href="https://blog.allenai.org/dolma-3-trillion-tokens-open-llm-corpus-9a0ff4b8da64">[blog]</a> <a href="https://techcrunch.com/2023/08/18/ai2-drops-biggest-open-dataset-yet-for-training-language-models/">[press]</a></sub>
<!-- <br><sub><a href="https://x.com/aclmeeting/status/1823664612577051026">[Best Resource Paper Award]</a></sub> -->
<br>
<sub>
  <a href="https://x.com/aclmeeting/status/1823664612577051026">
    <img src="https://img.shields.io/badge/Best%20Resource%20Paper%20Award-%F0%9F%8F%86-ADD8E6" alt="Best Resource Paper Award">
  </a>
</sub>

<br><br>

<br><em>What's In My Big Data?</em>
<br><sub>Yanai Elazar, Akshita Bhagia, <strong>Ian Magnusson</strong>, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr, Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, Hanna Hajishirzi, Noah A. Smith, Jesse Dodge</sub>
<br><sub> ICLR 2024 // <a href="https://arxiv.org/abs/2310.20707">[paper]</a> <a href="https://github.com/allenai/wimbd">[code]</a> <a href="https://wimbd.apps.allenai.org">[demo]</a> <a href="https://www.marktechpost.com/2023/11/05/peeking-inside-pandoras-box-unveiling-the-hidden-complexities-of-language-model-datasets-with-whats-in-my-big-data-wimbd/">[press]</a> </sub>
<!-- <br><sub><a href="https://iclr.cc/virtual/2024/events/spotlight-posters">[Spotlight]</a></sub> -->
<br>
<sub>
  <a href="https://iclr.cc/virtual/2024/events/spotlight-posters">
    <img src="https://img.shields.io/badge/Spotlight-%F0%9F%8F%86-ADD8E6" alt="Spotlight">
  </a>
</sub>

<br><br>

<br><em>Reproducibility in NLP: What Have We Learned from the Checklist?</em>
<br><sub><strong>Ian Magnusson</strong>, Noah A. Smith, Jesse Dodge</sub>
<br><sub> Findings of ACL 2023 // <a href="https://aclanthology.org/2023.findings-acl.809.pdf">[paper]</a> </sub>

<br><br>

<br><em>Extracting Fine-Grained Knowledge Graphs of Scientific Claims: Dataset and Transformer-Based Results</em>
<br><sub><strong>Ian Magnusson</strong>, Scott Friedman</sub>
<br><sub> EMNLP 2021 // <a href="https://aclanthology.org/2021.emnlp-main.381.pdf">[paper]</a> <a href="https://github.com/siftech/SciClaim">[data]</a></sub>
